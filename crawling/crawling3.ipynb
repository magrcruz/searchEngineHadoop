{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from unicodedata import normalize\n",
    "import json\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "global index\n",
    "global storaged\n",
    "global visited_urls\n",
    "global paginas_descubiertas\n",
    "\n",
    "visited_urls = set()\n",
    "paginas_descubiertas = []#clave pagina, valor es la clave\n",
    "index = 0\n",
    "storaged = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproccesText(text):\n",
    "    # Eliminar tildes de las letras con tilde y mantener la letra ñ intacta\n",
    "    text = re.sub(\n",
    "        r\"([^n\\u0300-\\u036f]|n(?!\\u0303(?![\\u0300-\\u036f])))[\\u0300-\\u036f]+\", r\"\\1\",\n",
    "        normalize(\"NFD\", text), 0, re.I\n",
    "    )\n",
    "    text = text.replace(\"n\\u0303\", \"ñ\")  # Restaurar la letra \"ñ\"\n",
    "\n",
    "    # Eliminar comandos para fórmulas matemáticas (por ejemplo, {\\displaystyle ...})\n",
    "    text = re.sub(r'{\\\\[^}]+}', '', text)\n",
    "\n",
    "    # Eliminar comandos y referencias (por ejemplo, [1], [2], [[Archivo:...])\n",
    "    text = re.sub(r'\\[\\d+\\]', '', text)  # Eliminar referencias numéricas\n",
    "    text = re.sub(r'\\[\\[Archivo:[^\\]]+\\]\\]', '', text)  # Eliminar referencias a archivos\n",
    "    text = re.sub(r'\\[\\[[a-zA-Z0-9\\s]+\\]\\]', '', text)  # Eliminar otras referencias en doble corchete\n",
    "\n",
    "    # Eliminar caracteres no alfabéticos, eliminar saltos de línea adicionales\n",
    "    text = re.sub(r'(\\s*\\n\\s*)+', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para extraer el contenido de los párrafos\n",
    "def extract_content(soup):\n",
    "    content_data = []\n",
    "    for paragraph in soup.find_all('p'):\n",
    "        text = paragraph.get_text()\n",
    "        text = preproccesText(text)\n",
    "        content_data.append(text)\n",
    "    return \" \".join(content_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manageLinks(soup):\n",
    "    # Definir una expresión regular para filtrar los enlaces no deseados\n",
    "    pattern = re.compile(r\"/wiki/.*:|/wiki/Wiki\")\n",
    "\n",
    "    # Extraer los enlaces salientes\n",
    "    outgoing_links = [a[\"href\"] for a in soup.find_all(\"a\", href=True) if a[\"href\"].startswith(\"/wiki/\") and not pattern.match(a[\"href\"])]\n",
    "    outgoing_links = [enlace.replace(\"/wiki/\", \"\") for enlace in outgoing_links]\n",
    "    return outgoing_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para realizar web scraping\n",
    "def scrape_wikipedia(url):\n",
    "    #start_time = time.time()  # Registrar el tiempo de inicio\n",
    "    global index\n",
    "    global visited_urls\n",
    "    global paginas_descubiertas\n",
    "\n",
    "    if url in visited_urls:\n",
    "        #print(f'URL repetida: {url}')\n",
    "        return\n",
    "\n",
    "    visited_urls.add(url)\n",
    "    \n",
    "    response = requests.get(\"https://es.wikipedia.org/wiki/\" + url)\n",
    "    if response.status_code == 200:\n",
    "        page_content = response.text\n",
    "        soup = BeautifulSoup(page_content, \"html.parser\")\n",
    "        \n",
    "        # Extraer el título de la página\n",
    "        title = preproccesText(soup.find(\"h1\").text)\n",
    "\n",
    "        # Extraer el contenido de títulos, subtítulos y párrafos\n",
    "        content = extract_content(soup)\n",
    "\n",
    "        # Extraer el snippet (primer párrafo, 200 primeras palabras)\n",
    "        snippet = content[:200] if content else \"\"\n",
    "        \n",
    "        # Extraer los enlaces salientes\n",
    "        outgoing_links = manageLinks(soup)\n",
    "        #los agrega a la lista de enlaces salientes\n",
    "        paginas_descubiertas.extend(outgoing_links)\n",
    "\n",
    "        # Imprimir el número de outgoing_links\n",
    "        #print(f'Página: {url}, outgoing_links: {len(outgoing_links)}')\n",
    "\n",
    "        # Guardar los datos en un diccionario\n",
    "        page_data = {\n",
    "            \"url\": url,\n",
    "            \"titulo\": title,\n",
    "            \"contenido\": content,\n",
    "            \"snippet\": snippet,\n",
    "            \"enlaces_salientes\": outgoing_links\n",
    "        }\n",
    "\n",
    "        str = json.dumps(page_data, separators=(',', ':'))\n",
    "        \n",
    "        return str\n",
    "\n",
    "        data.append(page_data)\n",
    "\n",
    "        for link in outgoing_links:\n",
    "            next_url = f\"https://es.wikipedia.org{link}\"\n",
    "            scrape_wikipedia(next_url, max_pages, current_depth + 1)\n",
    "\n",
    "    #end_time = time.time()  # Registrar el tiempo de finalización\n",
    "    #elapsed_time = end_time - start_time\n",
    "    #print(f'Tiempo para la página : {elapsed_time:.2f} segundos')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "paginas_descubiertas.append('Algoritmo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Algoritmo']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paginas_descubiertas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Página: Algoritmo, outgoing_links: 217\n",
      "URL repetida: Algoritmo\n",
      "URL repetida: Algoritmo\n",
      "URL repetida: Algoritmo\n",
      "Página: Logaritmo, outgoing_links: 240\n",
      "Página: Diagrama_de_flujo, outgoing_links: 73\n",
      "Página: Ada_Lovelace, outgoing_links: 143\n",
      "Página: Matem%C3%A1ticas, outgoing_links: 316\n",
      "Página: L%C3%B3gica, outgoing_links: 559\n",
      "Página: Ciencias_de_la_computaci%C3%B3n, outgoing_links: 307\n",
      "Página: Lat%C3%ADn_tard%C3%ADo, outgoing_links: 94\n",
      "Página: %C3%81rabe_cl%C3%A1sico, outgoing_links: 34\n",
      "Página: Empleador, outgoing_links: 37\n"
     ]
    }
   ],
   "source": [
    "# Índices iniciales y finales\n",
    "tam = 10\n",
    "\n",
    "real = 0\n",
    "i = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "real = 0\n",
    "i = 0\n",
    "tam = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:00<00:06,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Página: Algoritmo, outgoing_links: 217\n",
      "URL repetida: Algoritmo\n",
      "URL repetida: Algoritmo\n",
      "URL repetida: Algoritmo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:01<00:06,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Página: Logaritmo, outgoing_links: 240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:02<00:05,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Página: Diagrama_de_flujo, outgoing_links: 73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:03<00:06,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Página: Ada_Lovelace, outgoing_links: 143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:05<00:05,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Página: Matem%C3%A1ticas, outgoing_links: 316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:06<00:04,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Página: L%C3%B3gica, outgoing_links: 559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:07<00:03,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Página: Ciencias_de_la_computaci%C3%B3n, outgoing_links: 307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:08<00:02,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Página: Lat%C3%ADn_tard%C3%ADo, outgoing_links: 94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:09<00:00,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Página: %C3%81rabe_cl%C3%A1sico, outgoing_links: 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:09<00:00,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Página: Empleador, outgoing_links: 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "real_final = real + tam\n",
    "\n",
    "# Nombre del archivo de salida\n",
    "nombre_archivo = f\"paginas{real:03d}-{real_final-1:03d}.txt\"\n",
    "\n",
    "# Abre el archivo en modo de escritura\n",
    "with open(nombre_archivo, \"w\") as archivo:\n",
    "    # Utiliza tqdm para crear un indicador de progreso\n",
    "    with tqdm(total=real_final - real) as pbar:\n",
    "        while real < real_final:\n",
    "            if i < len(paginas_descubiertas):\n",
    "                url = paginas_descubiertas[i]\n",
    "                resultado = scrape_wikipedia(url)\n",
    "\n",
    "                if resultado is not None:\n",
    "                    archivo.write(resultado + \"\\n\")\n",
    "                    real += 1\n",
    "                    pbar.update(1)  # Actualiza el indicador de progreso en 1\n",
    "\n",
    "                i += 1\n",
    "            else:\n",
    "                break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
